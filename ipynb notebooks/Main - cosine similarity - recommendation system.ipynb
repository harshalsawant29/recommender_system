{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f61c9068",
   "metadata": {},
   "source": [
    "### Import all the necessary dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f27ef80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "edf55489",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title_x</th>\n",
       "      <th>average_rating</th>\n",
       "      <th>rating_number</th>\n",
       "      <th>price</th>\n",
       "      <th>parent_asin</th>\n",
       "      <th>rating</th>\n",
       "      <th>title_y</th>\n",
       "      <th>text</th>\n",
       "      <th>user_id</th>\n",
       "      <th>category_Grocery</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>instant compostable espresso capsules lungo me...</td>\n",
       "      <td>4.3</td>\n",
       "      <td>85</td>\n",
       "      <td>8.49</td>\n",
       "      <td>B0C2W77WJX</td>\n",
       "      <td>4</td>\n",
       "      <td>fresh tasting smelling slightly acidic light l...</td>\n",
       "      <td>happen instant pod dual coffee maker spots nes...</td>\n",
       "      <td>AF2BLE54TEMGZ546U763ZHZRXC4A</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>instant compostable espresso capsules lungo me...</td>\n",
       "      <td>4.3</td>\n",
       "      <td>85</td>\n",
       "      <td>8.49</td>\n",
       "      <td>B0C2W77WJX</td>\n",
       "      <td>5</td>\n",
       "      <td>dynamic flavor interesting flavor profile body...</td>\n",
       "      <td>tried leggero light roast lungo medium roast d...</td>\n",
       "      <td>AF2BLE54TEMGZ546U763ZHZRXC4A</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>instant compostable espresso capsules lungo me...</td>\n",
       "      <td>4.3</td>\n",
       "      <td>85</td>\n",
       "      <td>8.49</td>\n",
       "      <td>B0C2W77WJX</td>\n",
       "      <td>4</td>\n",
       "      <td>pricey much flavor</td>\n",
       "      <td>great roast ppl arent bitter heavy taste like ...</td>\n",
       "      <td>AEUDZQDVSZYCHEXQSXLB6NWQTMHA</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>edible markersfood coloring markersfood colori...</td>\n",
       "      <td>4.3</td>\n",
       "      <td>1193</td>\n",
       "      <td>8.99</td>\n",
       "      <td>B07PK9L29R</td>\n",
       "      <td>5</td>\n",
       "      <td>fun</td>\n",
       "      <td>much fun color cookies artist really work well...</td>\n",
       "      <td>AGECC4F4CDL2AVODIRNCF3V63BEQ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>edible markersfood coloring markersfood colori...</td>\n",
       "      <td>4.3</td>\n",
       "      <td>1193</td>\n",
       "      <td>8.99</td>\n",
       "      <td>B07PK9L29R</td>\n",
       "      <td>5</td>\n",
       "      <td>perfect touch</td>\n",
       "      <td>perfect adding creative touches taste coloring...</td>\n",
       "      <td>AFF6LERKD46F2RLIKAMQTAQPOIWA</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             title_x  average_rating   \n",
       "0  instant compostable espresso capsules lungo me...             4.3  \\\n",
       "1  instant compostable espresso capsules lungo me...             4.3   \n",
       "2  instant compostable espresso capsules lungo me...             4.3   \n",
       "3  edible markersfood coloring markersfood colori...             4.3   \n",
       "4  edible markersfood coloring markersfood colori...             4.3   \n",
       "\n",
       "   rating_number  price parent_asin  rating   \n",
       "0             85   8.49  B0C2W77WJX       4  \\\n",
       "1             85   8.49  B0C2W77WJX       5   \n",
       "2             85   8.49  B0C2W77WJX       4   \n",
       "3           1193   8.99  B07PK9L29R       5   \n",
       "4           1193   8.99  B07PK9L29R       5   \n",
       "\n",
       "                                             title_y   \n",
       "0  fresh tasting smelling slightly acidic light l...  \\\n",
       "1  dynamic flavor interesting flavor profile body...   \n",
       "2                                 pricey much flavor   \n",
       "3                                                fun   \n",
       "4                                      perfect touch   \n",
       "\n",
       "                                                text   \n",
       "0  happen instant pod dual coffee maker spots nes...  \\\n",
       "1  tried leggero light roast lungo medium roast d...   \n",
       "2  great roast ppl arent bitter heavy taste like ...   \n",
       "3  much fun color cookies artist really work well...   \n",
       "4  perfect adding creative touches taste coloring...   \n",
       "\n",
       "                        user_id  category_Grocery  \n",
       "0  AF2BLE54TEMGZ546U763ZHZRXC4A                 1  \n",
       "1  AF2BLE54TEMGZ546U763ZHZRXC4A                 1  \n",
       "2  AEUDZQDVSZYCHEXQSXLB6NWQTMHA                 1  \n",
       "3  AGECC4F4CDL2AVODIRNCF3V63BEQ                 1  \n",
       "4  AFF6LERKD46F2RLIKAMQTAQPOIWA                 1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"model_building.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b91e226f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(110596, 10)\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9baf32db-f4d6-4af9-9f68-439aab6615dd",
   "metadata": {},
   "source": [
    "## Data Preprocessing: Tokenization\n",
    "### Objective: Convert text data into tokens for further analysis.\n",
    "\n",
    "#### Steps:\n",
    "\n",
    "- Import Libraries: Utilize nltk for natural language processing tasks.\n",
    "- Download Necessary Resources: Ensure the ‘punkt’ tokenizer models are available.\n",
    "- Data Conversion: Cast the ‘text’ column to string type to avoid type-related errors.\n",
    "- Tokenization: Apply word_tokenize from nltk to split the text into individual words or tokens.\n",
    "- Store Tokens: Save the tokenized data in a new column ‘tokenized_reviews’ for easy access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22d29ce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Hrishikesh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title_x</th>\n",
       "      <th>average_rating</th>\n",
       "      <th>rating_number</th>\n",
       "      <th>price</th>\n",
       "      <th>parent_asin</th>\n",
       "      <th>rating</th>\n",
       "      <th>title_y</th>\n",
       "      <th>text</th>\n",
       "      <th>user_id</th>\n",
       "      <th>category_Grocery</th>\n",
       "      <th>tokenized_reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>instant compostable espresso capsules lungo me...</td>\n",
       "      <td>4.3</td>\n",
       "      <td>85</td>\n",
       "      <td>8.49</td>\n",
       "      <td>B0C2W77WJX</td>\n",
       "      <td>4</td>\n",
       "      <td>fresh tasting smelling slightly acidic light l...</td>\n",
       "      <td>happen instant pod dual coffee maker spots nes...</td>\n",
       "      <td>AF2BLE54TEMGZ546U763ZHZRXC4A</td>\n",
       "      <td>1</td>\n",
       "      <td>[happen, instant, pod, dual, coffee, maker, sp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>instant compostable espresso capsules lungo me...</td>\n",
       "      <td>4.3</td>\n",
       "      <td>85</td>\n",
       "      <td>8.49</td>\n",
       "      <td>B0C2W77WJX</td>\n",
       "      <td>5</td>\n",
       "      <td>dynamic flavor interesting flavor profile body...</td>\n",
       "      <td>tried leggero light roast lungo medium roast d...</td>\n",
       "      <td>AF2BLE54TEMGZ546U763ZHZRXC4A</td>\n",
       "      <td>1</td>\n",
       "      <td>[tried, leggero, light, roast, lungo, medium, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>instant compostable espresso capsules lungo me...</td>\n",
       "      <td>4.3</td>\n",
       "      <td>85</td>\n",
       "      <td>8.49</td>\n",
       "      <td>B0C2W77WJX</td>\n",
       "      <td>4</td>\n",
       "      <td>pricey much flavor</td>\n",
       "      <td>great roast ppl arent bitter heavy taste like ...</td>\n",
       "      <td>AEUDZQDVSZYCHEXQSXLB6NWQTMHA</td>\n",
       "      <td>1</td>\n",
       "      <td>[great, roast, ppl, arent, bitter, heavy, tast...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>edible markersfood coloring markersfood colori...</td>\n",
       "      <td>4.3</td>\n",
       "      <td>1193</td>\n",
       "      <td>8.99</td>\n",
       "      <td>B07PK9L29R</td>\n",
       "      <td>5</td>\n",
       "      <td>fun</td>\n",
       "      <td>much fun color cookies artist really work well...</td>\n",
       "      <td>AGECC4F4CDL2AVODIRNCF3V63BEQ</td>\n",
       "      <td>1</td>\n",
       "      <td>[much, fun, color, cookies, artist, really, wo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>edible markersfood coloring markersfood colori...</td>\n",
       "      <td>4.3</td>\n",
       "      <td>1193</td>\n",
       "      <td>8.99</td>\n",
       "      <td>B07PK9L29R</td>\n",
       "      <td>5</td>\n",
       "      <td>perfect touch</td>\n",
       "      <td>perfect adding creative touches taste coloring...</td>\n",
       "      <td>AFF6LERKD46F2RLIKAMQTAQPOIWA</td>\n",
       "      <td>1</td>\n",
       "      <td>[perfect, adding, creative, touches, taste, co...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             title_x  average_rating   \n",
       "0  instant compostable espresso capsules lungo me...             4.3  \\\n",
       "1  instant compostable espresso capsules lungo me...             4.3   \n",
       "2  instant compostable espresso capsules lungo me...             4.3   \n",
       "3  edible markersfood coloring markersfood colori...             4.3   \n",
       "4  edible markersfood coloring markersfood colori...             4.3   \n",
       "\n",
       "   rating_number  price parent_asin  rating   \n",
       "0             85   8.49  B0C2W77WJX       4  \\\n",
       "1             85   8.49  B0C2W77WJX       5   \n",
       "2             85   8.49  B0C2W77WJX       4   \n",
       "3           1193   8.99  B07PK9L29R       5   \n",
       "4           1193   8.99  B07PK9L29R       5   \n",
       "\n",
       "                                             title_y   \n",
       "0  fresh tasting smelling slightly acidic light l...  \\\n",
       "1  dynamic flavor interesting flavor profile body...   \n",
       "2                                 pricey much flavor   \n",
       "3                                                fun   \n",
       "4                                      perfect touch   \n",
       "\n",
       "                                                text   \n",
       "0  happen instant pod dual coffee maker spots nes...  \\\n",
       "1  tried leggero light roast lungo medium roast d...   \n",
       "2  great roast ppl arent bitter heavy taste like ...   \n",
       "3  much fun color cookies artist really work well...   \n",
       "4  perfect adding creative touches taste coloring...   \n",
       "\n",
       "                        user_id  category_Grocery   \n",
       "0  AF2BLE54TEMGZ546U763ZHZRXC4A                 1  \\\n",
       "1  AF2BLE54TEMGZ546U763ZHZRXC4A                 1   \n",
       "2  AEUDZQDVSZYCHEXQSXLB6NWQTMHA                 1   \n",
       "3  AGECC4F4CDL2AVODIRNCF3V63BEQ                 1   \n",
       "4  AFF6LERKD46F2RLIKAMQTAQPOIWA                 1   \n",
       "\n",
       "                                   tokenized_reviews  \n",
       "0  [happen, instant, pod, dual, coffee, maker, sp...  \n",
       "1  [tried, leggero, light, roast, lungo, medium, ...  \n",
       "2  [great, roast, ppl, arent, bitter, heavy, tast...  \n",
       "3  [much, fun, color, cookies, artist, really, wo...  \n",
       "4  [perfect, adding, creative, touches, taste, co...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ensure nltk is downloaded and imported\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Convert any non-string data to string\n",
    "df['text'] = df['text'].astype(str)\n",
    "\n",
    "# Tokenize the text data\n",
    "tokenized_data = df['text'].apply(lambda x: word_tokenize(x))\n",
    "\n",
    "df['tokenized_reviews'] = tokenized_data\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2492e9fd-e711-46be-972f-c3eae319dd15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title_x</th>\n",
       "      <th>average_rating</th>\n",
       "      <th>rating_number</th>\n",
       "      <th>price</th>\n",
       "      <th>parent_asin</th>\n",
       "      <th>rating</th>\n",
       "      <th>title_y</th>\n",
       "      <th>user_id</th>\n",
       "      <th>category_Grocery</th>\n",
       "      <th>tokenized_reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>instant compostable espresso capsules lungo me...</td>\n",
       "      <td>4.3</td>\n",
       "      <td>85</td>\n",
       "      <td>8.49</td>\n",
       "      <td>B0C2W77WJX</td>\n",
       "      <td>4</td>\n",
       "      <td>fresh tasting smelling slightly acidic light l...</td>\n",
       "      <td>AF2BLE54TEMGZ546U763ZHZRXC4A</td>\n",
       "      <td>1</td>\n",
       "      <td>[happen, instant, pod, dual, coffee, maker, sp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>instant compostable espresso capsules lungo me...</td>\n",
       "      <td>4.3</td>\n",
       "      <td>85</td>\n",
       "      <td>8.49</td>\n",
       "      <td>B0C2W77WJX</td>\n",
       "      <td>5</td>\n",
       "      <td>dynamic flavor interesting flavor profile body...</td>\n",
       "      <td>AF2BLE54TEMGZ546U763ZHZRXC4A</td>\n",
       "      <td>1</td>\n",
       "      <td>[tried, leggero, light, roast, lungo, medium, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>instant compostable espresso capsules lungo me...</td>\n",
       "      <td>4.3</td>\n",
       "      <td>85</td>\n",
       "      <td>8.49</td>\n",
       "      <td>B0C2W77WJX</td>\n",
       "      <td>4</td>\n",
       "      <td>pricey much flavor</td>\n",
       "      <td>AEUDZQDVSZYCHEXQSXLB6NWQTMHA</td>\n",
       "      <td>1</td>\n",
       "      <td>[great, roast, ppl, arent, bitter, heavy, tast...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>edible markersfood coloring markersfood colori...</td>\n",
       "      <td>4.3</td>\n",
       "      <td>1193</td>\n",
       "      <td>8.99</td>\n",
       "      <td>B07PK9L29R</td>\n",
       "      <td>5</td>\n",
       "      <td>fun</td>\n",
       "      <td>AGECC4F4CDL2AVODIRNCF3V63BEQ</td>\n",
       "      <td>1</td>\n",
       "      <td>[much, fun, color, cookies, artist, really, wo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>edible markersfood coloring markersfood colori...</td>\n",
       "      <td>4.3</td>\n",
       "      <td>1193</td>\n",
       "      <td>8.99</td>\n",
       "      <td>B07PK9L29R</td>\n",
       "      <td>5</td>\n",
       "      <td>perfect touch</td>\n",
       "      <td>AFF6LERKD46F2RLIKAMQTAQPOIWA</td>\n",
       "      <td>1</td>\n",
       "      <td>[perfect, adding, creative, touches, taste, co...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             title_x  average_rating   \n",
       "0  instant compostable espresso capsules lungo me...             4.3  \\\n",
       "1  instant compostable espresso capsules lungo me...             4.3   \n",
       "2  instant compostable espresso capsules lungo me...             4.3   \n",
       "3  edible markersfood coloring markersfood colori...             4.3   \n",
       "4  edible markersfood coloring markersfood colori...             4.3   \n",
       "\n",
       "   rating_number  price parent_asin  rating   \n",
       "0             85   8.49  B0C2W77WJX       4  \\\n",
       "1             85   8.49  B0C2W77WJX       5   \n",
       "2             85   8.49  B0C2W77WJX       4   \n",
       "3           1193   8.99  B07PK9L29R       5   \n",
       "4           1193   8.99  B07PK9L29R       5   \n",
       "\n",
       "                                             title_y   \n",
       "0  fresh tasting smelling slightly acidic light l...  \\\n",
       "1  dynamic flavor interesting flavor profile body...   \n",
       "2                                 pricey much flavor   \n",
       "3                                                fun   \n",
       "4                                      perfect touch   \n",
       "\n",
       "                        user_id  category_Grocery   \n",
       "0  AF2BLE54TEMGZ546U763ZHZRXC4A                 1  \\\n",
       "1  AF2BLE54TEMGZ546U763ZHZRXC4A                 1   \n",
       "2  AEUDZQDVSZYCHEXQSXLB6NWQTMHA                 1   \n",
       "3  AGECC4F4CDL2AVODIRNCF3V63BEQ                 1   \n",
       "4  AFF6LERKD46F2RLIKAMQTAQPOIWA                 1   \n",
       "\n",
       "                                   tokenized_reviews  \n",
       "0  [happen, instant, pod, dual, coffee, maker, sp...  \n",
       "1  [tried, leggero, light, roast, lungo, medium, ...  \n",
       "2  [great, roast, ppl, arent, bitter, heavy, tast...  \n",
       "3  [much, fun, color, cookies, artist, really, wo...  \n",
       "4  [perfect, adding, creative, touches, taste, co...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now as the data (text - reviews data) is tokenised it's time to drop it the main 'text' column\n",
    "df = df.drop('text', axis = 1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629a66d1",
   "metadata": {},
   "source": [
    "## TF-IDF Vectorization\n",
    "\n",
    "#### To convert the tokenized reviews into a numerical format that can be used for machine learning models, we use the `TfidfVectorizer` from `sklearn.feature_extraction.text`. This process is known as TF-IDF vectorization.\n",
    "\n",
    "### Code Explanation\n",
    "\n",
    "- **Tokenized Reviews**: We start with the `tokenized_reviews` column which contains the tokenized text data.\n",
    "- **String Conversion**: Each list of tokens is joined into a single string.\n",
    "- **Hyperparameters**: We define hyperparameters such as `ngram_range` to include unigrams, bigrams, trigrams and `min_df` to set the minimum document frequency for terms.\n",
    "- **TF-IDF Vectorizer Initialization**: The vectorizer is initialized with the defined hyperparameters.\n",
    "- **Fitting and Transforming**: The vectorizer is then fitted to the tokenized strings and transforms them into a TF-IDF matrix.\n",
    "- **Matrix Shape**: Finally, we print the shape of the TF-IDF matrix to understand the dimensions of our feature space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5eb372",
   "metadata": {},
   "source": [
    "## For UNIGRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c554ea5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of TF-IDF matrix: (110596, 1000)\n"
     ]
    }
   ],
   "source": [
    "# initialising for UNIGRAM parameter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Assuming 'tokenized_reviews' column contains the tokenized reviews\n",
    "tokenized_reviews = df['tokenized_reviews']\n",
    "\n",
    "# Convert tokenized reviews into strings\n",
    "tokenized_reviews_str = tokenized_reviews.apply(lambda x: ' '.join(x))\n",
    "\n",
    "# Initialize TF-IDF vectorizer with hyperparameters\n",
    "tfidf_vectorizer_unigram = TfidfVectorizer(min_df=5, max_features=1000, strip_accents='unicode', \n",
    "                                   analyzer='word', ngram_range=(1, 1), stop_words='english')\n",
    "\n",
    "# Fit and transform the tokenized reviews\n",
    "tfidf_matrix_unigram = tfidf_vectorizer_unigram.fit_transform(tokenized_reviews_str)\n",
    "\n",
    "# Print the shape of the TF-IDF matrix\n",
    "print(\"Shape of TF-IDF matrix:\", tfidf_matrix_unigram.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97be5feb",
   "metadata": {},
   "source": [
    "### Here’s a breakdown of what these vectors represent:\n",
    "\n",
    "- TF-IDF: Stands for Term Frequency-Inverse Document Frequency. It’s a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus.\n",
    "- Vector Format: Each vector is in the format (document_index, word_id) tf-idf_score.\n",
    "- document_index: The index of the document in the corpus.\n",
    "- word_id: The unique identifier for a word within the corpus.\n",
    "- tf-idf_score: The TF-IDF score for that word in the specified document.\n",
    "- Purpose: These vectors are used to convert text data into a numerical form that can be used for various tasks such as clustering, classification, and information retrieval.\n",
    "\n",
    "#### For example, the vector (0, 8688) 0.13354025856957072 means that in the first document of the corpus (index 0), the word with ID 8688 has a TF-IDF score of approximately 0.134. This score represents the relative importance of this word in that particular document compared to the entire corpus. High scores indicate more importance, and vice versa. The vectors are typically sparse, meaning most of the values are zero, as a word does not appear in most documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "324d617e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 833)\t0.19988324457419349\n",
      "  (0, 101)\t0.13618657213644478\n",
      "  (0, 685)\t0.13685877764510995\n",
      "  (0, 385)\t0.11787480024439587\n",
      "  (0, 876)\t0.07708812427502859\n",
      "  (0, 335)\t0.16514864102056648\n",
      "  (0, 802)\t0.11348320174931192\n",
      "  (0, 172)\t0.10221784605550495\n",
      "  (0, 43)\t0.1328151749357382\n",
      "  (0, 558)\t0.12413250395585446\n",
      "  (0, 86)\t0.13107646919734461\n",
      "  (0, 851)\t0.09219194785467436\n",
      "  (0, 149)\t0.1182761181879643\n",
      "  (0, 223)\t0.10472181958737305\n",
      "  (0, 911)\t0.12407589991831008\n",
      "  (0, 676)\t0.20267375568361673\n",
      "  (0, 482)\t0.3223766461398797\n",
      "  (0, 742)\t0.3324244229481629\n",
      "  (0, 531)\t0.48620797378900976\n",
      "  (0, 483)\t0.04927763825732075\n",
      "  (0, 892)\t0.09855283356708616\n",
      "  (0, 642)\t0.1328584149446982\n",
      "  (0, 978)\t0.11064549756638102\n",
      "  (0, 307)\t0.09370128609582781\n",
      "  (0, 93)\t0.07095592053032089\n",
      "  :\t:\n",
      "  (110593, 108)\t0.43299344806133416\n",
      "  (110593, 267)\t0.32073230128299807\n",
      "  (110593, 69)\t0.25582266740755594\n",
      "  (110593, 846)\t0.3177553455710735\n",
      "  (110593, 936)\t0.2397595869171905\n",
      "  (110593, 600)\t0.2973272132423977\n",
      "  (110593, 714)\t0.23509346575218562\n",
      "  (110593, 678)\t0.24917976436712735\n",
      "  (110593, 873)\t0.17449298848271183\n",
      "  (110593, 483)\t0.3308016549667909\n",
      "  (110594, 90)\t0.3794958636902057\n",
      "  (110594, 313)\t0.3709622208867908\n",
      "  (110594, 104)\t0.3807235190732906\n",
      "  (110594, 648)\t0.30642662617370564\n",
      "  (110594, 568)\t0.35534989013039847\n",
      "  (110594, 150)\t0.3210360235303309\n",
      "  (110594, 469)\t0.31579992141482605\n",
      "  (110594, 954)\t0.26058047520657507\n",
      "  (110594, 112)\t0.20610020733299722\n",
      "  (110594, 255)\t0.20018866652725856\n",
      "  (110595, 190)\t0.5683558199119958\n",
      "  (110595, 50)\t0.44989609608659037\n",
      "  (110595, 879)\t0.3524070896241281\n",
      "  (110595, 369)\t0.4957943996449359\n",
      "  (110595, 507)\t0.32336097655959917\n"
     ]
    }
   ],
   "source": [
    "print(tfidf_matrix_unigram)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d311ae1",
   "metadata": {},
   "source": [
    "### Sparse Matrix Conversion\n",
    "The code snippet is converting a dense TF-IDF matrix to a sparse matrix format using the `csr_matrix` class from the `scipy.sparse` module. This is done to optimize memory usage when dealing with large matrices.\n",
    "\n",
    "### Output Explanation\n",
    "- **Type of tfidf_matrix_sparse**: Confirms the matrix is now in CSR format.\n",
    "- **Shape of tfidf_matrix_sparse**: The matrix dimensions reflect the number of documents (rows) and the maximum number of features (columns) considered in the TF-IDF vectorization.\n",
    "- **Number of non-zero entries**: Indicates the total count of non-zero values in the matrix, which represents the actual data points that are stored in memory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a63151f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of tfidf_matrix_sparse: <class 'scipy.sparse._csr.csr_matrix'>\n",
      "Shape of tfidf_matrix_sparse: (110596, 1000)\n",
      "Number of non-zero entries: 1334939\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "# Convert to a sparse matrix (if not already)\n",
    "tfidf_matrix_sparse_unigram = csr_matrix(tfidf_matrix_unigram)\n",
    "\n",
    "# Print the type and shape of the sparse TF-IDF matrix\n",
    "print(\"Type of tfidf_matrix_sparse:\", type(tfidf_matrix_sparse_unigram))\n",
    "print(\"Shape of tfidf_matrix_sparse:\", tfidf_matrix_sparse_unigram.shape)\n",
    "print(\"Number of non-zero entries:\", tfidf_matrix_sparse_unigram.nnz)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e78194",
   "metadata": {},
   "source": [
    "#### Model building using sparsed vectors -  UNIGRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53f6ebae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import normalize\n",
    "from scipy.sparse.linalg import svds\n",
    "\n",
    "\n",
    "# Normalize the TF-IDF matrix\n",
    "normalized_tfidf_matrix_sparse_unigram = normalize(tfidf_matrix_sparse_unigram, norm='l2', axis=1)\n",
    "\n",
    "# Initialize Truncated SVD with desired number of components\n",
    "n_components = 100  # Adjust the number of components as needed\n",
    "# Compute Truncated SVD\n",
    "u, s, vt = svds(normalized_tfidf_matrix_sparse_unigram, k=n_components)\n",
    "\n",
    "# Construct the reduced TF-IDF matrix\n",
    "tfidf_matrix_reduced_unigram = np.dot(u, np.diag(s))\n",
    "\n",
    "\n",
    "# Get recommendations based on cosine similarity matrix in chunks\n",
    "def get_recommendations_in_chunks_unigram(product_index, matrix, n=5, threshold=0.2, chunk_size=1000):\n",
    "    num_docs = matrix.shape[0]\n",
    "    sim_scores = np.zeros(num_docs)\n",
    "\n",
    "    for chunk_start in range(0, num_docs, chunk_size):\n",
    "        chunk_end = min(chunk_start + chunk_size, num_docs)\n",
    "        sim_chunk = cosine_similarity(matrix[product_index:product_index+1], \n",
    "                                      matrix[chunk_start:chunk_end])\n",
    "        sim_scores[chunk_start:chunk_end] = sim_chunk.flatten()\n",
    "\n",
    "    # Filter out low similarity scores based on threshold\n",
    "    sim_indices = [(idx, score) for idx, score in enumerate(sim_scores) if score > threshold]\n",
    "    # Sort the products based on similarity scores\n",
    "    sim_indices = sorted(sim_indices, key=lambda x: x[1], reverse=True)\n",
    "    # Get the top similar products\n",
    "    top_similar_products = sim_indices[:n]  # Limit to top N\n",
    "    return top_similar_products"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a860ff0",
   "metadata": {},
   "source": [
    "#### Getting recommendation for the above mode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49441b71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 recommended products:\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------+------------+-------------------------+\n",
      "|                                                                                 Title                                                                                 | Price | Avg Rating | Cosine Similarity Score |\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------+------------+-------------------------+\n",
      "| instant compostable espresso capsules lungo medium roast 10 plantbased capsules makers instant pot ecofriendly 100 organic arabica capsules compostable freshness bag | 8.49  |    4.3     |           1.0           |\n",
      "|                   holland valley coffee keurig kcup coffee maker compatible high caffeine roast 100 organic coffee single serve pods usda approved                    | 15.0  |    4.1     |   0.8923242173539396    |\n",
      "|                                     bean coffee company organic il chicco traditional italian roast dark roast ground 16ounce bag                                     | 14.99 |    4.2     |   0.8918875799091318    |\n",
      "|                                                      copper moon whole bean coffee dark roast sumatra blend 5 lb                                                      | 37.45 |    4.5     |   0.8679719483086412    |\n",
      "|                                        wolfgang puck soft coffee pods sorrento swiss water process decaf blend 18count pack 3                                         | 10.45 |    3.3     |   0.8629890335603778    |\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------+------------+-------------------------+\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "# Example usage\n",
    "product_index = 0\n",
    "top_similar_products = get_recommendations_in_chunks_unigram(product_index, \n",
    "                                                             tfidf_matrix_reduced_unigram)\n",
    "\n",
    "# Prepare the data for tabular display\n",
    "table_data = []\n",
    "for index, score in top_similar_products:\n",
    "    title = df.iloc[index]['title_x']\n",
    "    price = df.iloc[index]['price']\n",
    "    avg_rating = df.iloc[index]['average_rating']\n",
    "    table_data.append([title, price, avg_rating, score])\n",
    "\n",
    "# Display the recommendations in a tabular format\n",
    "print(\"Top 5 recommended products:\")\n",
    "print(tabulate(table_data, headers=['Title', 'Price', 'Avg Rating', \n",
    "                                    'Cosine Similarity Score'], tablefmt='pretty'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e635be08",
   "metadata": {},
   "source": [
    "##### NDGC Score - UNIGRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cd58b8eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG Score: 0.9875426533550047\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import ndcg_score\n",
    "\n",
    "# Extract the indices and scores from the recommendations\n",
    "top_indices_unigram = [index for index, score in top_similar_products]\n",
    "top_scores_unigram = [score for index, score in top_similar_products]\n",
    "\n",
    "# Assuming 'df' is your DataFrame and 'average_rating' is a column in your dataset\n",
    "# Prepare relevance scores based on the average_rating of the recommended products\n",
    "relevance_scores = df.loc[top_indices_unigram, 'average_rating'].tolist()\n",
    "\n",
    "# Normalize relevance scores since NDCG in sklearn assumes relevance scores, not ratings directly\n",
    "max_rating = max(relevance_scores)\n",
    "normalized_relevance_scores = [score / max_rating for score in relevance_scores]\n",
    "\n",
    "# Reshape for ndcg_score function\n",
    "true_relevance = np.asarray([normalized_relevance_scores])\n",
    "predicted_relevance = np.asarray([top_scores_unigram])\n",
    "\n",
    "# Calculate NDCG score\n",
    "ndcg = ndcg_score(true_relevance, predicted_relevance)\n",
    "print(\"NDCG Score:\", ndcg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4eb559c",
   "metadata": {},
   "source": [
    "### Using bi gram\n",
    "#### For this model, all the above pre-processing and feature engineering were performed similarly below for bigram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "57c1f5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "\n",
    "# Initialize TF-IDF vectorizer to use only bigrams\n",
    "tfidf_vectorizer_bigram = TfidfVectorizer(min_df=5, max_features=1000, strip_accents='unicode', \n",
    "                                           analyzer='word', ngram_range=(1, 2), stop_words='english')\n",
    "\n",
    "# Fit and transform the tokenized reviews\n",
    "tfidf_matrix_bigram = tfidf_vectorizer_bigram.fit_transform(tokenized_reviews_str)\n",
    "\n",
    "# Convert to a sparse matrix\n",
    "tfidf_matrix_sparse_bigram = csr_matrix(tfidf_matrix_bigram)\n",
    "\n",
    "# Normalize the TF-IDF matrix\n",
    "normalized_tfidf_matrix_sparse_bigram = normalize(tfidf_matrix_sparse_bigram)\n",
    "\n",
    "# Initialize Truncated SVD\n",
    "n_components = 100\n",
    "svd = TruncatedSVD(n_components=n_components)\n",
    "\n",
    "# Apply Truncated SVD\n",
    "tfidf_matrix_reduced_bigram = svd.fit_transform(normalized_tfidf_matrix_sparse_bigram)\n",
    "\n",
    "# Function to get recommendations based on cosine similarity\n",
    "def get_recommendations_in_chunks(product_index, matrix, n=5, threshold=0.2, chunk_size=1000):\n",
    "    num_docs = matrix.shape[0]\n",
    "    sim_scores = np.zeros(num_docs)\n",
    "\n",
    "    for chunk_start in range(0, num_docs, chunk_size):\n",
    "        chunk_end = min(chunk_start + chunk_size, num_docs)\n",
    "        sim_chunk = cosine_similarity(matrix[product_index:product_index+1], matrix[chunk_start:chunk_end])\n",
    "        sim_scores[chunk_start:chunk_end] = sim_chunk.flatten()\n",
    "\n",
    "    sim_indices = np.argwhere(sim_scores > threshold).flatten()\n",
    "    top_similar_products_bigram = sorted([(idx, sim_scores[idx]) for idx in sim_indices], key=lambda x: x[1], reverse=True)[:n]\n",
    "    return top_similar_products_bigram\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1da9c3",
   "metadata": {},
   "source": [
    "#### Getting recommendation and NDGC score- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3e4f6b7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 recommended products:\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------+------------+-------------------------+\n",
      "|                                                                                 Title                                                                                 | Price | Avg Rating | Cosine Similarity Score |\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------+------------+-------------------------+\n",
      "| instant compostable espresso capsules lungo medium roast 10 plantbased capsules makers instant pot ecofriendly 100 organic arabica capsules compostable freshness bag | 8.49  |    4.3     |   0.9999999999999999    |\n",
      "|                   holland valley coffee keurig kcup coffee maker compatible high caffeine roast 100 organic coffee single serve pods usda approved                    | 15.0  |    4.1     |   0.8942084499448029    |\n",
      "|                                     bean coffee company organic il chicco traditional italian roast dark roast ground 16ounce bag                                     | 14.99 |    4.2     |   0.8660705483951944    |\n",
      "|                                        brooklyn beans expresso gourmet coffee pods compatible 20 keurig k cup brewers 40 count                                        | 23.98 |    4.3     |   0.8548599293871814    |\n",
      "|                                 amazon brand solimo medium roast coffee pods donut style compatible keurig 20 kcup brewers 100 count                                  | 28.82 |    4.4     |   0.8435356470111102    |\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------+------------+-------------------------+\n",
      "NDCG Score: 0.9907518530040953\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "product_index = 0\n",
    "top_similar_products_bigram = get_recommendations_in_chunks(product_index, tfidf_matrix_reduced_bigram)\n",
    "\n",
    "# Prepare the data for tabular display\n",
    "table_data = []\n",
    "for index, score in top_similar_products_bigram:\n",
    "    title = df.iloc[index]['title_x']\n",
    "    price = df.iloc[index]['price']\n",
    "    avg_rating = df.iloc[index]['average_rating']\n",
    "    table_data.append([title, price, avg_rating, score])\n",
    "\n",
    "# Display the recommendations in a tabular format\n",
    "print(\"Top 5 recommended products:\")\n",
    "print(tabulate(table_data, headers=['Title', 'Price', 'Avg Rating', 'Cosine Similarity Score'], tablefmt='pretty'))\n",
    "\n",
    "# Evaluation\n",
    "# product_index = 0\n",
    "top_similar_products_bigram = get_recommendations_in_chunks(product_index, tfidf_matrix_reduced_bigram)\n",
    "\n",
    "top_indices_bigram = [index for index, score in top_similar_products_bigram]\n",
    "top_scores_bigram = [score for index, score in top_similar_products_bigram]\n",
    "relevance_scores = df.loc[top_indices_bigram, 'average_rating'].tolist()\n",
    "\n",
    "max_rating = max(relevance_scores)\n",
    "normalized_relevance_scores = [score / max_rating for score in relevance_scores]\n",
    "\n",
    "true_relevance = np.asarray([normalized_relevance_scores])\n",
    "predicted_relevance = np.asarray([top_scores_bigram])\n",
    "\n",
    "ndcg = ndcg_score(true_relevance, predicted_relevance)\n",
    "print(\"NDCG Score:\", ndcg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d752b8cd",
   "metadata": {},
   "source": [
    "### For Trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0fff9a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize TF-IDF vectorizer to use only trigrams\n",
    "tfidf_vectorizer_trigram = TfidfVectorizer(min_df=5, max_features=1000, strip_accents='unicode', \n",
    "                                            analyzer='word', ngram_range=(1, 3), stop_words='english')\n",
    "\n",
    "# Fit and transform the tokenized reviews\n",
    "tfidf_matrix_trigram = tfidf_vectorizer_trigram.fit_transform(tokenized_reviews_str)\n",
    "\n",
    "# Convert to a sparse matrix\n",
    "tfidf_matrix_sparse_trigram = csr_matrix(tfidf_matrix_trigram)\n",
    "\n",
    "# Normalize the TF-IDF matrix\n",
    "normalized_tfidf_matrix_sparse_trigram = normalize(tfidf_matrix_sparse_trigram)\n",
    "\n",
    "# Initialize Truncated SVD\n",
    "n_components = 100\n",
    "svd = TruncatedSVD(n_components=n_components)\n",
    "\n",
    "# Apply Truncated SVD\n",
    "tfidf_matrix_reduced_trigram = svd.fit_transform(normalized_tfidf_matrix_sparse_trigram)\n",
    "\n",
    "# Function to get recommendations based on cosine similarity\n",
    "def get_recommendations_in_chunks(product_index, matrix, n=5, threshold=0.2, chunk_size=1000):\n",
    "    num_docs = matrix.shape[0]\n",
    "    sim_scores = np.zeros(num_docs)\n",
    "\n",
    "    for chunk_start in range(0, num_docs, chunk_size):\n",
    "        chunk_end = min(chunk_start + chunk_size, num_docs)\n",
    "        sim_chunk = cosine_similarity(matrix[product_index:product_index+1], matrix[chunk_start:chunk_end])\n",
    "        sim_scores[chunk_start:chunk_end] = sim_chunk.flatten()\n",
    "\n",
    "    sim_indices = np.argwhere(sim_scores > threshold).flatten()\n",
    "    top_similar_products_trigram = sorted([(idx, sim_scores[idx]) for idx in sim_indices], key=lambda x: x[1], reverse=True)[:n]\n",
    "    return top_similar_products_trigram\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c2b3017b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 recommended products:\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------+------------+-------------------------+\n",
      "|                                                                                 Title                                                                                 | Price | Avg Rating | Cosine Similarity Score |\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------+------------+-------------------------+\n",
      "| instant compostable espresso capsules lungo medium roast 10 plantbased capsules makers instant pot ecofriendly 100 organic arabica capsules compostable freshness bag | 8.49  |    4.3     |   1.0000000000000004    |\n",
      "|                   holland valley coffee keurig kcup coffee maker compatible high caffeine roast 100 organic coffee single serve pods usda approved                    | 15.0  |    4.1     |   0.8777333886900638    |\n",
      "|                                     bean coffee company organic il chicco traditional italian roast dark roast ground 16ounce bag                                     | 14.99 |    4.2     |   0.8576588520582484    |\n",
      "|                                                      copper moon whole bean coffee dark roast sumatra blend 5 lb                                                      | 37.45 |    4.5     |   0.8354185199304819    |\n",
      "|                                                         fara coffee ground signature roast medium dark 12 oz                                                          | 11.99 |    4.7     |   0.8339813882373621    |\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------+------------+-------------------------+\n",
      "NDCG Score: 0.9739335011467453\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "product_index = 0\n",
    "top_similar_products_trigram = get_recommendations_in_chunks(product_index, tfidf_matrix_reduced_trigram)\n",
    "\n",
    "# Prepare the data for tabular display\n",
    "table_data = []\n",
    "for index, score in top_similar_products_trigram:\n",
    "    title = df.iloc[index]['title_x']\n",
    "    price = df.iloc[index]['price']\n",
    "    avg_rating = df.iloc[index]['average_rating']\n",
    "    table_data.append([title, price, avg_rating, score])\n",
    "\n",
    "# Display the recommendations in a tabular format\n",
    "print(\"Top 5 recommended products:\")\n",
    "print(tabulate(table_data, headers=['Title', 'Price', 'Avg Rating', 'Cosine Similarity Score'], tablefmt='pretty'))\n",
    "\n",
    "# Evaluation\n",
    "\n",
    "top_similar_products_trigram = get_recommendations_in_chunks(product_index, tfidf_matrix_reduced_trigram)\n",
    "\n",
    "top_indices_trigram = [index for index, score in top_similar_products_trigram]\n",
    "top_scores_trigram = [score for index, score in top_similar_products_trigram]\n",
    "relevance_scores = df.loc[top_indices_trigram, 'average_rating'].tolist()\n",
    "\n",
    "max_rating = max(relevance_scores)\n",
    "normalized_relevance_scores = [score / max_rating for score in relevance_scores]\n",
    "\n",
    "true_relevance = np.asarray([normalized_relevance_scores])\n",
    "predicted_relevance = np.asarray([top_scores_trigram])\n",
    "\n",
    "ndcg = ndcg_score(true_relevance, predicted_relevance)\n",
    "print(\"NDCG Score:\", ndcg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebad708",
   "metadata": {},
   "source": [
    "## Gensim\n",
    "### Another way to build recommendation model is through gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f904240",
   "metadata": {},
   "source": [
    "since i was encountering this error - Unable to allocate 52.8 GiB for an array with shape (110596, 64024) and data type float64\n",
    "i'm using gensim library,  Gensim has an efficient TF-IDF model that doesn’t require loading the entire corpus into memory at once. Instead, it operates on an iterable representation of your data.\n",
    "\n",
    "Here’s how you can use Gensim for TF-IDF representation:\n",
    "\n",
    "Install Gensim (if you haven’t already):\n",
    "pip install gensim\n",
    "\n",
    "Prepare Your Corpus:\n",
    "Ensure your corpus is an iterable (e.g., a list of tokenized reviews).\n",
    "You can use Python generators to achieve this.\n",
    "Create a Gensim Dictionary:\n",
    "Create a dictionary from your tokenized reviews.\n",
    "The dictionary maps words to unique integer IDs.\n",
    "Create a Gensim Corpus:\n",
    "Convert your tokenized reviews into a Gensim corpus using the dictionary.\n",
    "The corpus is a list of bag-of-words representations (sparse vectors) for each document.\n",
    "Compute TF-IDF:\n",
    "Use Gensim’s TfidfModel to compute the TF-IDF scores based on the corpus.\n",
    "Access the TF-IDF Vectors:\n",
    "You can access the TF-IDF vectors for individual documents without loading the entire matrix into memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca0abfe",
   "metadata": {},
   "source": [
    "### Thoughts behind using gensim and corpora - \n",
    "- Importing Gensim: We start by importing Gensim, a versatile Python library renowned for its capabilities in natural language processing tasks such as topic modeling, document similarity analysis, and text summarization. This step ensures we have access to the tools necessary for our task ahead.\n",
    "- Creating a Dictionary: In this step, we construct a dictionary that serves as a fundamental component in the subsequent processes. This dictionary acts as a mapping between words and their unique integer IDs within our corpus of product reviews. By establishing this mapping, we lay the groundwork for efficient representation and analysis of textual data.\n",
    "- Creating a Corpus: With our dictionary in place, we proceed to create a corpus, essentially a collection of documents represented in a format suitable for computational analysis. Each document is transformed into a bag-of-words vector, where each word's frequency within the document is recorded. This step transforms our raw textual data into a structured format amenable to further processing.\n",
    "- TF-IDF Model: Building upon the bag-of-words representation, we leverage Gensim's capabilities to construct a TF-IDF model. TF-IDF (Term Frequency-Inverse Document Frequency) is a statistical measure used to evaluate the importance of a word in a document relative to a corpus. By applying this model to our corpus, we compute TF-IDF scores for each word, capturing their significance within individual documents and across the entire corpus.\n",
    "- Accessing TF-IDF Vectors: Here, we access the TF-IDF vectors generated by our model, enabling us to explore and utilize the transformed textual data. These vectors encapsulate the essence of each document's content, with each dimension representing a unique word and its corresponding TF-IDF score. By retrieving these vectors, we gain valuable insights into the semantic composition of our product reviews, paving the way for various downstream applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "335b1dc5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 0.15822019336594337), (1, 0.07911009668297168), (2, 0.12170821532633551), (3, 0.07493418887314095), (4, 0.11030375926386878), (5, 0.10859182482154411), (6, 0.04947903549083479), (7, 0.10374842137816317), (8, 0.1136241355227434), (9, 0.13596692814909372), (10, 0.13138885661596642), (11, 0.09599582425095556), (12, 0.1565601201334325), (13, 0.08020523861709881), (14, 0.08266694420118104), (15, 0.11723536349072858), (16, 0.18658037825369134), (17, 0.0888713835981412), (18, 0.07936684071231864), (19, 0.07183341245268707), (20, 0.12707538562583412), (21, 0.07401393015048931), (22, 0.032219935791116754), (23, 0.12179428611948441), (24, 0.05987914001758319), (25, 0.120831002481178), (26, 0.09560107520762093), (27, 0.06796589385054766), (28, 0.10209478190637414), (29, 0.11026127062439335), (30, 0.2560739659343325), (31, 0.028176078444046855), (32, 0.11693806256431222), (33, 0.1019816764207001), (34, 0.11392707128848491), (35, 0.1524549702782134), (36, 0.3968735655113111), (37, 0.10175734360716843), (38, 0.1205468458267122), (39, 0.14494580166562673), (40, 0.17899439864420652), (41, 0.11034633725911869), (42, 0.12399966044774859), (43, 0.11944849764833426), (44, 0.15867835234489372), (45, 0.11428632229866684), (46, 0.1485431044461317), (47, 0.2659537149160561), (48, 0.12847559962027527), (49, 0.09128183907513461), (50, 0.14230128775197218), (51, 0.1559351244263627), (52, 0.07034985227279414), (53, 0.12529514467356573), (54, 0.055505412590967515), (55, 0.07660234476498358), (56, 0.10170164665517933), (57, 0.058133556316320174), (58, 0.08849130632677189)], [(0, 0.11416457669124039), (1, 0.057082288345620194), (2, 0.1756383504151153), (4, 0.07959023254817132), (5, 0.07835497763681423), (6, 0.07140369407635799), (8, 0.08198606674598023), (9, 0.09810762119502503), (10, 0.18960858128664884), (11, 0.06926627004171028), (14, 0.059648749570531484), (20, 0.045845941972449275), (22, 0.04649691360323611), (24, 0.021603047396522624), (25, 0.08718621786497685), (28, 0.07366700362808785), (29, 0.07955957465655392), (30, 0.43113340361148905), (31, 0.04066118236809315), (36, 0.14318305913628962), (38, 0.08698118320106354), (43, 0.08618866454603892), (44, 0.11449516360796967), (46, 0.1071820244857923), (47, 0.4477672296616153), (48, 0.09270221539850006), (51, 0.11251577369936086), (52, 0.05076128990968353), (53, 0.09040734212757148), (57, 0.04194656008957726), (59, 0.048030471594611615), (60, 0.04125404222963803), (61, 0.055342676638254575), (62, 0.05870789929889441), (63, 0.10425053064574136), (64, 0.08765834628045986), (65, 0.06361296319487746), (66, 0.09120075773748264), (67, 0.08888106172324096), (68, 0.08087250080371218), (69, 0.09379640966047523), (70, 0.03405476603602033), (71, 0.04996449255495275), (72, 0.06409368616979012), (73, 0.0955170407805), (74, 0.1461461743812108), (75, 0.054149599195664394), (76, 0.10738634222187224), (77, 0.1536085602864444), (78, 0.04682556039124491), (79, 0.2601942374819704), (80, 0.20941787593289363), (81, 0.15113035449017623), (82, 0.03143056760768222), (83, 0.07228210868878054), (84, 0.07734709665396505), (85, 0.06703817110006498), (86, 0.06425392704512127), (87, 0.08367058294033307), (88, 0.12698023221598426), (89, 0.12323108012601793), (90, 0.047019053021239245), (91, 0.08899746457015016), (92, 0.10139468938281304), (93, 0.09347533124138291), (94, 0.04212494348851259), (95, 0.03228047043631043)], [(22, 0.059435528639504104), (31, 0.05197589865377), (47, 0.16353332444188057), (53, 0.23112967101845402), (61, 0.14148557344635723), (66, 0.23315806699364935), (71, 0.1277360494741096), (96, 0.16753711121382117), (97, 0.19727257776792978), (98, 0.13869761031581512), (99, 0.26028130020535134), (100, 0.33794243734996116), (101, 0.17620205218625346), (102, 0.1581279374627826), (103, 0.14563163126522014), (104, 0.26652028789486004), (105, 0.07933663480085813), (106, 0.1962888392659639), (107, 0.19890252861837474), (108, 0.15938361993832514), (109, 0.18280014949331955), (110, 0.33259803404708693), (111, 0.09722253555362718), (112, 0.3257888503586839), (113, 0.056886161162777525), (114, 0.11680398930026273), (115, 0.14192080692811235)], [(57, 0.14714870634845528), (105, 0.10886339464803592), (115, 0.19473955319838818), (116, 0.5241167345705715), (117, 0.46312326779036983), (118, 0.2147881412031032), (119, 0.26934176660823517), (120, 0.12337660264907935), (121, 0.12296654558415389), (122, 0.5388594650414585)], [(3, 0.232654913903139), (113, 0.09574528300880353), (123, 0.28223500802093854), (124, 0.4060912910436275), (125, 0.4769925179797686), (126, 0.29322123326761207), (127, 0.23476133370226132), (128, 0.5687920162545391)], [(70, 0.10139706025857939), (129, 0.18392019026419607), (130, 0.16359835555657576), (131, 0.1951919848113645), (132, 0.13788735301426827), (133, 0.11962204628105651), (134, 0.10473082480338733), (135, 0.15795967604798405), (136, 0.27277867311113235), (137, 0.1645904498555037), (138, 0.1631441257157089), (139, 0.19964288987582682), (140, 0.13047513642393369), (141, 0.21940228987999869), (142, 0.14351792159564555), (143, 0.3791180752579176), (144, 0.22838224896242873), (145, 0.5051505415496846), (146, 0.29051909515877405), (147, 0.18052173063526142)], [(6, 0.043166148684837584), (12, 0.13658506793342023), (19, 0.06266839544692261), (28, 0.17813760886802033), (78, 0.05661553308258239), (148, 0.13500232264754844), (149, 0.09899648696849205), (150, 0.14391415548664527), (151, 0.1707584161302298), (152, 0.0671406183730905), (153, 0.18064275453849296), (154, 0.15615702183726118), (155, 0.09645447660395232), (156, 0.10486155973036489), (157, 0.12645258671423348), (158, 0.16839988818787707), (159, 0.1857240031726577), (160, 0.15221569919704678), (161, 0.16277512828365348), (162, 0.07713789819859809), (163, 0.19288562088910888), (164, 0.18064275453849296), (165, 0.20512848723972477), (166, 0.09456723643375421), (167, 0.08801256614762654), (168, 0.2533183412394839), (169, 0.17348113682204178), (170, 0.14758163198139668), (171, 0.12545442722251762), (172, 0.18064275453849296), (173, 0.17348113682204178), (174, 0.12751055097145647), (175, 0.0646636528142584), (176, 0.09147687721269282), (177, 0.13112777786597052), (178, 0.20512848723972477), (179, 0.041795160943454514), (180, 0.06867812601598983), (181, 0.15407665275497473), (182, 0.09568241368812605), (183, 0.09320289237414721), (184, 0.1428433596105722), (185, 0.21163023716845428), (186, 0.040176367004863774), (187, 0.06190280952912888), (188, 0.16895330144111895), (189, 0.1343480729422073), (190, 0.08810601992421778), (191, 0.05411136978470962), (192, 0.07718832713384043), (193, 0.10217343791851013), (194, 0.17348113682204178), (195, 0.05055641624800683), (196, 0.10111486961133685)], [(70, 0.1509496322911158), (105, 0.13755485842037654), (197, 0.2875815034401167), (198, 0.17822786624059955), (199, 0.3793157875272854), (200, 0.5276003253191869), (201, 0.6029177073726111), (202, 0.24098183376090862)], [(203, 0.5490947594964163), (204, 0.3018565936175597), (205, 0.350260591151986), (206, 0.3249014435357001), (207, 0.4794010402625605), (208, 0.24893736516291362), (209, 0.2955316278816479)], [(23, 0.10959111430586523), (113, 0.11099248836856532), (127, 0.13607325489831845), (140, 0.10929222338395803), (143, 0.15878372886063516), (147, 0.15121364768029927), (202, 0.13559357485837867), (210, 0.1611763563170237), (211, 0.2836432736270073), (212, 0.18487109556396963), (213, 0.21354561020957816), (214, 0.15437817183678681), (215, 0.22110197096081133), (216, 0.11501021075291555), (217, 0.3650524848235283), (218, 0.1223310269301099), (219, 0.09544826782747899), (220, 0.17275505581684597), (221, 0.14377140549585818), (222, 0.1804162814086015), (223, 0.24994313515184197), (224, 0.1628922353642523), (225, 0.2739624853167774), (226, 0.23274030301843568), (227, 0.2612720705641176), (228, 0.19472627830766778), (229, 0.14662388003518012), (230, 0.14240013896124945)]]\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim import corpora, models\n",
    "\n",
    "# Assuming 'tokenized_reviews' is an iterable (e.g., list of lists)\n",
    "dictionary = corpora.Dictionary(tokenized_reviews)\n",
    "corpus = [dictionary.doc2bow(review) for review in tokenized_reviews]\n",
    "\n",
    "# Create the TF-IDF model\n",
    "tfidf_model = models.TfidfModel(corpus)\n",
    "\n",
    "# Create a list to hold all TF-IDF vectors\n",
    "all_tfidf_vectors = []\n",
    "\n",
    "# Iterate over all documents in the corpus\n",
    "for doc in corpus:\n",
    "    # Apply the TF-IDF model to get the vector for the current document\n",
    "    doc_tfidf_vector = tfidf_model[doc]\n",
    "    # Append the vector to the list\n",
    "    all_tfidf_vectors.append(doc_tfidf_vector)\n",
    "\n",
    "# Now 'all_tfidf_vectors' contains the TF-IDF vectors for all documents\n",
    "# Limit the output to the first 10 TF-IDF vectors\n",
    "limited_tfidf_vectors = all_tfidf_vectors[:10]\n",
    "\n",
    "# Now 'limited_tfidf_vectors' contains the TF-IDF vectors for the first 10 documents\n",
    "print(limited_tfidf_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594816af",
   "metadata": {},
   "source": [
    "#### Applying padding on the vectors to obtain it in equal lengths \n",
    "- Importing NumPy: The code begins by importing the NumPy library, which is a fundamental package for scientific computing in Python.\n",
    "- Max Length Calculation: max_length is determined by finding the longest vector in all_tfidf_vectors, which is a list of TF-IDF vectors. This ensures that all vectors will be padded to the same length.\n",
    "- Padding Function: pad_vector is a function that takes a vector and the max_length as arguments. It calculates the necessary padding (a list of zeros) to make the vector’s length equal to max_length. The padding is then appended to the original vector, and the padded vector is returned.\n",
    "- Applying Padding: The list comprehension applies the pad_vector function to each vector in all_tfidf_vectors. The result is padded_vectors, a new list where each TF-IDF vector has been padded with zeros to have the same length, ensuring uniformity for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a50b118a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the maximum length among all vectors\n",
    "max_length = max(len(vector) for vector in all_tfidf_vectors)\n",
    "\n",
    "# Convert TF-IDF vectors to NumPy arrays\n",
    "all_tfidf_arrays = [np.array([(0, 0)] * max_length) for vector in all_tfidf_vectors]\n",
    "\n",
    "\n",
    "# Function to convert TF-IDF vectors to arrays\n",
    "def convert_to_array(vector, max_length):\n",
    "    array = np.array(vector)\n",
    "    result = np.zeros((max_length, 2))  # Assuming TF-IDF vectors are represented as tuples (index, value)\n",
    "    result[:len(array)] = array\n",
    "    return result\n",
    "\n",
    "# Apply conversion and padding to each vector\n",
    "padded_vectors = [convert_to_array(vector, max_length) for vector in all_tfidf_vectors]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a1205dd7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[0.        , 0.15822019],\n",
      "       [1.        , 0.0791101 ],\n",
      "       [2.        , 0.12170822],\n",
      "       ...,\n",
      "       [0.        , 0.        ],\n",
      "       [0.        , 0.        ],\n",
      "       [0.        , 0.        ]]), array([[0.        , 0.11416458],\n",
      "       [1.        , 0.05708229],\n",
      "       [2.        , 0.17563835],\n",
      "       ...,\n",
      "       [0.        , 0.        ],\n",
      "       [0.        , 0.        ],\n",
      "       [0.        , 0.        ]]), array([[22.        ,  0.05943553],\n",
      "       [31.        ,  0.0519759 ],\n",
      "       [47.        ,  0.16353332],\n",
      "       ...,\n",
      "       [ 0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ]]), array([[5.70000000e+01, 1.47148706e-01],\n",
      "       [1.05000000e+02, 1.08863395e-01],\n",
      "       [1.15000000e+02, 1.94739553e-01],\n",
      "       ...,\n",
      "       [0.00000000e+00, 0.00000000e+00],\n",
      "       [0.00000000e+00, 0.00000000e+00],\n",
      "       [0.00000000e+00, 0.00000000e+00]]), array([[3.00000000e+00, 2.32654914e-01],\n",
      "       [1.13000000e+02, 9.57452830e-02],\n",
      "       [1.23000000e+02, 2.82235008e-01],\n",
      "       ...,\n",
      "       [0.00000000e+00, 0.00000000e+00],\n",
      "       [0.00000000e+00, 0.00000000e+00],\n",
      "       [0.00000000e+00, 0.00000000e+00]])]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "limited_padded_vectors = padded_vectors[:5]\n",
    "\n",
    "# Now 'limited_tfidf_vectors' contains the TF-IDF vectors for the first 5 documents\n",
    "print(limited_padded_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03f45cf",
   "metadata": {},
   "source": [
    "### Model Building and its steps\n",
    "- Normalization: The TF-IDF matrix, stored in tfidf_matrix_sparse, is normalized using sklearn's normalize function. Normalization ensures that each TF-IDF vector has a unit norm, which can improve the performance of subsequent operations.\n",
    "- Dimensionality Reduction: Truncated Singular Value Decomposition (SVD) is applied to the normalized TF-IDF matrix to reduce its dimensionality while preserving important information. This step is crucial for managing computational complexity and capturing the most relevant features.\n",
    "- Chunk-based Similarity Calculation: To handle large datasets efficiently, the code defines a function chunk_similarity that calculates cosine similarity scores between a reference document and chunks of the TF-IDF matrix. This approach enables memory-efficient computation by processing the data in manageable chunks.\n",
    "- Recommendation Generation: Another function, get_recommendations_in_chunks, utilizes the chunk-based similarity calculation to generate recommendations for a given product index. It calculates cosine similarity scores between the reference product and all other products in the dataset, filtering out low similarity scores based on a threshold. Finally, it returns the top N products with the highest similarity scores as recommendations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5b2a48",
   "metadata": {},
   "source": [
    "#### Model building using Gensim method - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2dbfb5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "we need to reshape the padded_vectors array to collapse the last two dimensions \n",
    "into one before passing it to the normalize function.\n",
    "'''\n",
    "# Convert the list of padded vectors to a NumPy array\n",
    "padded_vectors_array = np.array(padded_vectors)\n",
    "\n",
    "# Reshape padded_vectors to collapse the last two dimensions into one\n",
    "reshaped_vectors = padded_vectors_array.reshape(padded_vectors_array.shape[0], -1)\n",
    "\n",
    "# Normalize the reshaped TF-IDF matrix\n",
    "normalized_tfidf_matrix_sparse = normalize(reshaped_vectors)\n",
    "\n",
    "# Initialize Truncated SVD with desired number of components\n",
    "n_components = 100  # Adjust the number of components as needed\n",
    "svd = TruncatedSVD(n_components=n_components)\n",
    "\n",
    "# Apply Truncated SVD to the normalized TF-IDF matrix\n",
    "tfidf_matrix_reduced = svd.fit_transform(normalized_tfidf_matrix_sparse)\n",
    "\n",
    "\n",
    "# Get recommendations based on cosine similarity matrix in chunks\n",
    "def get_recommendations_in_chunks(product_index, matrix, n=5, threshold=0.2, chunk_size=1000):\n",
    "    num_docs = matrix.shape[0]\n",
    "    sim_scores = np.zeros(num_docs)\n",
    "\n",
    "    for chunk_start in range(0, num_docs, chunk_size):\n",
    "        chunk_end = min(chunk_start + chunk_size, num_docs)\n",
    "        sim_chunk = cosine_similarity(matrix[product_index:product_index+1], matrix[chunk_start:chunk_end])\n",
    "        sim_scores[chunk_start:chunk_end] = sim_chunk.flatten()\n",
    "\n",
    "    # Filter out low similarity scores based on threshold\n",
    "    sim_indices = [(idx, score) for idx, score in enumerate(sim_scores) if score > threshold]\n",
    "    # Sort the products based on similarity scores\n",
    "    sim_indices = sorted(sim_indices, key=lambda x: x[1], reverse=True)\n",
    "    # Get the top similar products\n",
    "    top_similar_products = sim_indices[:n]  # Limit to top N\n",
    "    return top_similar_products"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b67e91",
   "metadata": {},
   "source": [
    "### Getting Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fc3fd22a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 recommended products:\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------+------------+-------------------------+\n",
      "|                                                                                 Title                                                                                 | Price | Avg Rating | Cosine Similarity Score |\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------+------------+-------------------------+\n",
      "| instant compostable espresso capsules lungo medium roast 10 plantbased capsules makers instant pot ecofriendly 100 organic arabica capsules compostable freshness bag | 8.49  |    4.3     |           1.0           |\n",
      "|                                    artisana organics raw almond butter 14oz sugar added palm oil vegan paleo keto friendly non gmo                                    | 19.49 |    4.4     |   0.9675793494807188    |\n",
      "|                                  san francisco bay whole bean coffee decaf gourmet blend 2lb bag medium roast swiss water processed                                   | 22.99 |    4.4     |    0.964745432959697    |\n",
      "|              chaga mushroom elixir four sigmatic coffee alternative organic chaga mushroom powder rose hips mint immune support overall wellness pack 20              | 27.54 |    4.4     |    0.960428762201994    |\n",
      "|                                                   nature valley chewy fruit nut granola bars trail mix 12 oz 15 ct                                                    | 7.48  |    4.7     |   0.9514508465594764    |\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------+------------+-------------------------+\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "product_index = 0\n",
    "top_similar_products = get_recommendations_in_chunks(product_index, tfidf_matrix_reduced)\n",
    "\n",
    "# Prepare the data for tabular display\n",
    "table_data = []\n",
    "for index, score in top_similar_products:\n",
    "    title = df.iloc[index]['title_x']\n",
    "    price = df.iloc[index]['price']\n",
    "    avg_rating = df.iloc[index]['average_rating']\n",
    "    table_data.append([title, price, avg_rating, score])\n",
    "\n",
    "# Display the recommendations in a tabular format\n",
    "print(\"Top 5 recommended products:\")\n",
    "print(tabulate(table_data, headers=['Title', 'Price', 'Avg Rating', 'Cosine Similarity Score'], tablefmt='pretty'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2f909b",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d9627cdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG Score: 0.9814682661666775\n"
     ]
    }
   ],
   "source": [
    "# Assuming get_recommendations_in_chunks is defined and returns a list of tuples (index, score)\n",
    "product_index = 0\n",
    "top_similar_products = get_recommendations_in_chunks(product_index, tfidf_matrix_reduced)\n",
    "\n",
    "# Extract the indices and scores from the recommendations\n",
    "top_indices = [index for index, score in top_similar_products]\n",
    "top_scores = [score for index, score in top_similar_products]\n",
    "\n",
    "# Assuming 'df' is your DataFrame and 'average_rating' is a column in dataset\n",
    "# Prepare relevance scores based on the average_rating of the recommended products\n",
    "relevance_scores = df.loc[top_indices, 'average_rating'].tolist()\n",
    "\n",
    "# Normalize relevance scores since NDCG in sklearn assumes relevance scores, not ratings directly\n",
    "max_rating = max(relevance_scores)\n",
    "normalized_relevance_scores = [score / max_rating for score in relevance_scores]\n",
    "\n",
    "# Reshape for ndcg_score function\n",
    "true_relevance = np.asarray([normalized_relevance_scores])\n",
    "predicted_relevance = np.asarray([top_scores])\n",
    "\n",
    "# Calculate NDCG score\n",
    "ndcg = ndcg_score(true_relevance, predicted_relevance)\n",
    "print(\"NDCG Score:\", ndcg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5d39df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
